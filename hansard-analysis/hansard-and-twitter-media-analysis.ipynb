{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88de2f8e",
   "metadata": {},
   "source": [
    "# Hansard and Twitter Analysis\n",
    "The goal of this project was to explore the relationship between debates in parliament, and political tweets. To do this, data was scraped using requests and BeautifulSoup from the website https://openparliament.ca/, and from twitter using the twitter API. Finally the data is compared using plotly express."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfb4a29",
   "metadata": {},
   "source": [
    "## Importing Modules\n",
    "To get the data we need, first we must import the modules. You may see that for some modules we need to install them as they do not come preinstalled.\n",
    "\n",
    "This is done with the code: \n",
    "\n",
    "`try:\n",
    "    import module\n",
    "except:\n",
    "    !pip conda module\n",
    "    import module`\n",
    "\n",
    "Below you will see the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a930da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import requests\n",
    "except:\n",
    "    !pip conda requests\n",
    "    import requests\n",
    "\n",
    "try:\n",
    "    import bs4\n",
    "    from bs4 import BeautifulSoup\n",
    "except:\n",
    "    !pip conda bs4\n",
    "    import bs4\n",
    "    from bs4 import BeautifulSoup\n",
    "    \n",
    "try:\n",
    "    import spacy\n",
    "except:\n",
    "    !pip conda spacy\n",
    "    !python -m spacy download en_core_web_sm\n",
    "    import spacy\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "try:\n",
    "    import twitter\n",
    "except:\n",
    "    !pip conda twitter\n",
    "    import twitter\n",
    "\n",
    "import json\n",
    "import re\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d93bb3",
   "metadata": {},
   "source": [
    "## Getting Hansard Data\n",
    "### Making Soup\n",
    "To get the Hansard Data we will be scraping from the website https://openparliament.ca. To do this, we use the requests module to send a request. It returns the HTML markup for the web page. To understand the markup, we will be using BS4. This is a module that sorts through the markup and allows us to pull specific data that we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9523c171",
   "metadata": {},
   "outputs": [],
   "source": [
    "dateOfDebate = ('2022/10/18/')\n",
    "page = requests.get('https://openparliament.ca/debates/' + dateOfDebate).content\n",
    "data = BeautifulSoup(page, 'html.parser')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dde111e",
   "metadata": {},
   "source": [
    "### Storing The Data\n",
    "We will store the data we find in two ways: first, a string of everything said, and second, a list of every time someone spoke filled with objects that contain their party, name, and what was said. Both are defined in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2d387f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class speaker:\n",
    "  def __init__(self, name, party, said):\n",
    "    self.name = name\n",
    "    self.party = party\n",
    "    self.said = said\n",
    "speakersList = []\n",
    "\n",
    "text = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67954272",
   "metadata": {},
   "source": [
    "### Getting Data\n",
    "To get our data we have to use BS4, to sort through all the data on the webpage. First we look at the markup (this can be done by inspecting the webpage). From there we need to find our data and then use a method from BS4 to retrieve it. I used .findAll() to get the list of every time someone spoke and .find() to get data about the person who spoke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808a4e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data.findAll(\"div\", class_=\"row statement_browser statement\"):\n",
    "    name = i.find('span', class_='pol_name').text\n",
    "    try:\n",
    "        party = i.find('p', class_='partytag').text\n",
    "    except AttributeError:\n",
    "        party = 'N/A\\n'\n",
    "    said = i.find('div', class_='text').text\n",
    "    speakersList.append(speaker(name, party, said))\n",
    "    text = text + said"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ddc412",
   "metadata": {},
   "source": [
    "Now we have all of our data from the debate. From here we get the twitter data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9296ff54",
   "metadata": {},
   "source": [
    "## Getting Twitter Data\n",
    "### Limits To The Twitter API\n",
    "There are many limits to my current access to the Twitter API. I only have Essential access. The limits can be read about here: https://developer.twitter.com/en/docs/twitter-api/getting-started/about-twitter-api\n",
    "\n",
    "Ideally, I would be able to access the timestamps of tweets to allow me to make time graphs. In addition, being able to search by country of accounts would allow me to filter out tweets from other countries, but I do not have that ability. Instead I have limited the hashtags we search to ones that are explicitly canadian. So #conservative is not included, but #conservativepartyofcanada is. Finally, the amount of tweets I can gather from each hashtag is limited, so this limits the sample size. All in all, this project is a demonstration of potential rather than an in depth analysis. \n",
    "\n",
    "### Hashtags\n",
    "Below are the hashtags we are searching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89a24c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtagsList = [\"canadianpolitics\", \"canada\", \"cdnpoli\", \"justintrudeau\", \"trudeaumustgo\", \"trudeau\", \n",
    "    \"canadian\", \"ontario\", \"toronto\", \"ottawa\", \"alberta\", \"andrewscheer\", \"canadapolitics\", \"cpc\",\n",
    "    \"conservativepartyofcanada\", \"fucktrudeau\", \"cdnpolitics\", \"canadavotes\", \"ppc\", \"britishcolumbia\",\n",
    "    \"ndp\", \"cbc\", \"liberalpartyofcanada\", \"quebec\", \"politicscanada\", \"canadianelection\", \"erinotoole\", \n",
    "    \"canadiannews\", \"canpoli\", \"canadaelection\", \"montreal\", \"jagmeetsingh\", \"makecanadagreatagain\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9496af1c",
   "metadata": {},
   "source": [
    "### Setup Of The Request\n",
    "Below is the setup for the request. It has my unique bearer token that connects my code to my app. This allows us to make requests through the twitter API. In addition, we have the URL and the headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca3a80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bearer_token = \"AAAAAAAAAAAAAAAAAAAAABSNhwEAAAAA0wvbzndD61OQwAPE62nNyw7QsE8%3DWs28ejwV2Fvq8334rl4uqIsJjHpdOkIySQK1caUVBC4oTePGbN\"\n",
    "url = \"https://api.twitter.com/2/tweets/search/recent?query=\"\n",
    "headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24807d1",
   "metadata": {},
   "source": [
    "### Getting The Data\n",
    "Here is where we connect to the twitter API and request the top tweets from each hashtag listed above. We then use json to understand the response and pull out the text from each tweet. It also keeps count of the number of tweets being pulled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc94ab8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweetText = ''\n",
    "tweetAmount = 0\n",
    "for hashtag in hashtagsList:\n",
    "    response = requests.request(\"GET\", url + hashtag, headers=headers).json()\n",
    "    try:\n",
    "        for tweet in response['data']:\n",
    "            tweetText = tweetText + tweet['text']\n",
    "            tweetAmount += 1\n",
    "    except KeyError:\n",
    "        continue\n",
    "print(\"Amount Of Tweets Used: \" + str(tweetAmount))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8e446b",
   "metadata": {},
   "source": [
    "### Filtering The Data\n",
    "The text data that we get from the twitter API is a massive list of every word tweeted. Many of the words like @mentions, urls and hashtags we are not interested in, including such things, would throw off our data because the same hashtags and @mentions are used in many tweets saying different things.\n",
    "\n",
    "To fix this we use RegEx to filter the unwanted text out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739dd525",
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredTweetText = ' '.join(re.sub(\"(#[A-Za-z0-9]+)|(@[A-Za-z0-9]+)\",\" \", tweetText).split())\n",
    "print(\"Text Filtered!\\n@mentions, and hashtags have been removed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e0fe93",
   "metadata": {},
   "source": [
    "Now we have all our data from both Twitter and the debate. From here we can start to analyze it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dfb6bf",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "### Natural Language Processing\n",
    "To process what is being said, we are going to use spacy to filter the text. In this case we are looking for common nouns.\n",
    "\n",
    "Below we have a function that returns the most common nouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaba215",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def getNouns(textData, amount):\n",
    "    #nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(textData)\n",
    "    nouns = [token.text\n",
    "             for token in doc\n",
    "             if (not token.is_stop and\n",
    "                 not token.is_punct and\n",
    "                 token.pos_ == \"NOUN\")]\n",
    "    nounFreq = Counter(nouns)\n",
    "    commonNouns = nounFreq.most_common(amount)\n",
    "    nounList = []\n",
    "    countList = []\n",
    "    for i in commonNouns:\n",
    "        nounList.append(i[0])\n",
    "        countList.append(i[1])\n",
    "    return(nounList, countList)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98363022",
   "metadata": {},
   "source": [
    "## Filtering Common Nouns\n",
    "Below you can see the most common nouns across both the Hansard transcript and the Twitter data. This is done using the function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2add28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweetNouns = getNouns(filteredTweetText, 10)\n",
    "print(tweetNouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cdc7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "debateNouns = getNouns(text, 10)\n",
    "# debateNounsList = debateNouns[0]\n",
    "# debateValueDict = {'Count' : debateNouns[1]}\n",
    "\n",
    "print(debateNouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d98d76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fig = px.bar(debateValueDict, title='Common Nouns in the House of Commons', labels={'index':'Noun', 'value':'Count'}).update_layout(showlegend=False)\n",
    "fig = px.bar(x=debateNouns[0], y=debateNouns[1], \n",
    "             title='Common Nouns in the House of Commons', \n",
    "             labels={'x':'Noun', 'y':'Count'}).update_layout(showlegend=False)\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
